{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offense target identification\n",
    "\n",
    "This is the task 'c' in Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media in SemEval 2019. We have explored how a RNN with attention + cyclic learning rate can be used in Offense target identification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (3876, 5)\n",
      "Test shape :  (213, 3)\n"
     ]
    }
   ],
   "source": [
    "# Filter the non offensive posts in the training set\n",
    "train = pd.read_csv(\"Data/training/offenseval-training-v1.tsv\", sep='\\t')\n",
    "train = train.loc[train['subtask_a'] == 'OFF']\n",
    "train = train.loc[train['subtask_b'] == 'TIN']\n",
    "\n",
    "test_tweets = pd.read_csv(\"Data/testing/test_set_taskc.tsv\", sep='\\t')\n",
    "test_labels = pd.read_csv(\"Data/testing/labels-test-c.csv\", header=-1, names = [\"id\", \"subtask_c\"])\n",
    "\n",
    "test = pd.merge(test_tweets, test_labels, on=['id','id'])\n",
    "print(\"Train shape : \", train.shape)\n",
    "print(\"Test shape : \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the texts to lower case since the embedding model only has lower case words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tweet\"] = train[\"tweet\"].str.lower()\n",
    "test[\"tweet\"] = test[\"tweet\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "train[\"tweet\"] = train[\"tweet\"].apply(lambda x: clean_text(x))\n",
    "test[\"tweet\"] = test[\"tweet\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 72 # max number of words in a question to use #99.99%\n",
    "\n",
    "## fill up the missing values\n",
    "X = train[\"tweet\"].fillna(\"_na_\").values\n",
    "X_test = test[\"tweet\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "## Pad the sentences \n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "Y = train['subtask_c'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(Y)\n",
    "encoded_Y = le.transform(Y)\n",
    "encoded_Y\n",
    "\n",
    "num_classes = np.max(encoded_Y) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)+1\n",
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '/data/glove/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '/data/fasttext/crawl-300d-2M-subword.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Cyclic learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr()) \n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the model with Bi directional GRU and LSTM with self attention on each. It is followed by a average pooling and max pooling layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm_atten():\n",
    "    \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    x = Bidirectional(LSTM(40, return_sequences=True))(x)\n",
    "    y = Bidirectional(GRU(40, return_sequences=True))(x)\n",
    "    \n",
    "    atten_1 = Attention(maxlen)(x) # skip connect\n",
    "    atten_2 = Attention(maxlen)(y)\n",
    "    avg_pool = GlobalAveragePooling1D()(y)\n",
    "    max_pool = GlobalMaxPooling1D()(y)\n",
    "    \n",
    "    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n",
    "    conc = Dense(16, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(num_classes, activation=\"sigmoid\")(conc)    \n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/strideradu/word2vec-and-gensim-go-go-go\n",
    "def train_pred(model, train_X, train_y, val_X, val_y, epochs=2, callback=None):\n",
    "    for e in range(epochs):\n",
    "        model.fit(train_X, train_y, batch_size=64, epochs=20, validation_data=(val_X, val_y), callbacks = callback, verbose=1)\n",
    "        pred_val_y = model.predict([val_X], batch_size=64, verbose=0)\n",
    "\n",
    "    pred_test_y = model.predict([X_test], batch_size=64, verbose=1)\n",
    "    return pred_val_y, pred_test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the embedding matrix, Only glove is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_1 = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9927, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = embedding_matrix_1\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      " 320/2906 [==>...........................] - ETA: 24s - loss: 0.5859 - f1: 0.5513"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tharindu/anaconda3/envs/sentence_similarity_3.6/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.363179). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2906/2906 [==============================] - 7s 2ms/step - loss: 0.5236 - f1: 0.5935 - val_loss: 0.4935 - val_f1: 0.6236\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.4614 - f1: 0.6578 - val_loss: 0.4331 - val_f1: 0.6806\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.4304 - f1: 0.6956 - val_loss: 0.4219 - val_f1: 0.7007\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.4084 - f1: 0.7164 - val_loss: 0.4173 - val_f1: 0.7114\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3925 - f1: 0.7361 - val_loss: 0.4150 - val_f1: 0.7310\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3749 - f1: 0.7455 - val_loss: 0.4148 - val_f1: 0.7181\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3608 - f1: 0.7641 - val_loss: 0.4354 - val_f1: 0.7110\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3423 - f1: 0.7799 - val_loss: 0.4343 - val_f1: 0.7175\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3147 - f1: 0.8001 - val_loss: 0.4460 - val_f1: 0.7019\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2912 - f1: 0.8211 - val_loss: 0.4693 - val_f1: 0.6914\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2570 - f1: 0.8436 - val_loss: 0.4733 - val_f1: 0.6965\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2326 - f1: 0.8566 - val_loss: 0.5409 - val_f1: 0.7002\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2019 - f1: 0.8800 - val_loss: 0.5509 - val_f1: 0.6892\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1730 - f1: 0.9039 - val_loss: 0.6027 - val_f1: 0.6699\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1566 - f1: 0.9133 - val_loss: 0.6502 - val_f1: 0.6836\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1429 - f1: 0.9180 - val_loss: 0.7073 - val_f1: 0.6892\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1299 - f1: 0.9275 - val_loss: 0.7126 - val_f1: 0.6904\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1290 - f1: 0.9298 - val_loss: 0.7189 - val_f1: 0.6879\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1078 - f1: 0.9408 - val_loss: 0.8101 - val_f1: 0.6620\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1035 - f1: 0.9437 - val_loss: 0.7672 - val_f1: 0.6965\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0870 - f1: 0.9550 - val_loss: 0.8433 - val_f1: 0.7004\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0691 - f1: 0.9674 - val_loss: 0.8601 - val_f1: 0.6881\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0529 - f1: 0.9766 - val_loss: 0.9467 - val_f1: 0.6946\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0605 - f1: 0.9688 - val_loss: 1.0205 - val_f1: 0.6940\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0508 - f1: 0.9745 - val_loss: 0.9867 - val_f1: 0.6831\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0342 - f1: 0.9830 - val_loss: 1.0162 - val_f1: 0.6778\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0191 - f1: 0.9923 - val_loss: 1.1044 - val_f1: 0.6758\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0202 - f1: 0.9912 - val_loss: 1.2245 - val_f1: 0.6798\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0199 - f1: 0.9914 - val_loss: 1.2377 - val_f1: 0.6928\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0259 - f1: 0.9871 - val_loss: 1.1937 - val_f1: 0.6890\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0221 - f1: 0.9891 - val_loss: 1.2499 - val_f1: 0.6831\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0296 - f1: 0.9852 - val_loss: 1.1735 - val_f1: 0.6557\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0447 - f1: 0.9805 - val_loss: 1.0916 - val_f1: 0.6687\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0345 - f1: 0.9807 - val_loss: 1.1573 - val_f1: 0.6970\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0304 - f1: 0.9859 - val_loss: 1.1710 - val_f1: 0.6728\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0334 - f1: 0.9852 - val_loss: 1.1681 - val_f1: 0.6645\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0291 - f1: 0.9831 - val_loss: 1.2385 - val_f1: 0.6943\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0217 - f1: 0.9893 - val_loss: 1.2201 - val_f1: 0.6771\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0151 - f1: 0.9922 - val_loss: 1.2660 - val_f1: 0.6876\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0074 - f1: 0.9955 - val_loss: 1.3546 - val_f1: 0.6770\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0087 - f1: 0.9950 - val_loss: 1.3798 - val_f1: 0.6743\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0139 - f1: 0.9943 - val_loss: 1.4174 - val_f1: 0.6797\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0124 - f1: 0.9938 - val_loss: 1.4241 - val_f1: 0.6762\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0150 - f1: 0.9924 - val_loss: 1.3406 - val_f1: 0.6767\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0180 - f1: 0.9919 - val_loss: 1.4075 - val_f1: 0.6880\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0172 - f1: 0.9909 - val_loss: 1.3332 - val_f1: 0.6847\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0236 - f1: 0.9890 - val_loss: 1.3326 - val_f1: 0.6918\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0173 - f1: 0.9916 - val_loss: 1.3167 - val_f1: 0.7050\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0137 - f1: 0.9921 - val_loss: 1.3522 - val_f1: 0.6786\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0135 - f1: 0.9931 - val_loss: 1.3400 - val_f1: 0.6846\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0048 - f1: 0.9985 - val_loss: 1.4348 - val_f1: 0.6816\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0071 - f1: 0.9954 - val_loss: 1.4711 - val_f1: 0.6798\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0084 - f1: 0.9964 - val_loss: 1.5051 - val_f1: 0.6908\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9976 - val_loss: 1.5255 - val_f1: 0.6910\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0063 - f1: 0.9979 - val_loss: 1.5420 - val_f1: 0.6672\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0080 - f1: 0.9950 - val_loss: 1.5597 - val_f1: 0.6860\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0131 - f1: 0.9960 - val_loss: 1.4875 - val_f1: 0.6697\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0067 - f1: 0.9962 - val_loss: 1.5530 - val_f1: 0.6970\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0170 - f1: 0.9936 - val_loss: 1.4950 - val_f1: 0.6845\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0268 - f1: 0.9869 - val_loss: 1.3511 - val_f1: 0.6959\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0155 - f1: 0.9916 - val_loss: 1.3671 - val_f1: 0.6939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0177 - f1: 0.9921 - val_loss: 1.4016 - val_f1: 0.6817\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0071 - f1: 0.9974 - val_loss: 1.3965 - val_f1: 0.6653\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0101 - f1: 0.9941 - val_loss: 1.4697 - val_f1: 0.6992\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0058 - f1: 0.9974 - val_loss: 1.4862 - val_f1: 0.6881\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0041 - f1: 0.9978 - val_loss: 1.5700 - val_f1: 0.6808\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0058 - f1: 0.9967 - val_loss: 1.5777 - val_f1: 0.7073\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9986 - val_loss: 1.5659 - val_f1: 0.7047\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0022 - f1: 0.9995 - val_loss: 1.6265 - val_f1: 0.6926\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0067 - f1: 0.9960 - val_loss: 1.6577 - val_f1: 0.6809\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0110 - f1: 0.9947 - val_loss: 1.5512 - val_f1: 0.6866\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0183 - f1: 0.9909 - val_loss: 1.5322 - val_f1: 0.6814\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0168 - f1: 0.9931 - val_loss: 1.4038 - val_f1: 0.6838\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0113 - f1: 0.9957 - val_loss: 1.4977 - val_f1: 0.6973\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9964 - val_loss: 1.4199 - val_f1: 0.6964\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0087 - f1: 0.9967 - val_loss: 1.4681 - val_f1: 0.6939\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0057 - f1: 0.9979 - val_loss: 1.5794 - val_f1: 0.7077\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0034 - f1: 0.9983 - val_loss: 1.5700 - val_f1: 0.6956\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0096 - f1: 0.9959 - val_loss: 1.5598 - val_f1: 0.7026\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0121 - f1: 0.9943 - val_loss: 1.5392 - val_f1: 0.6995\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0153 - f1: 0.9930 - val_loss: 1.5463 - val_f1: 0.7031\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0053 - f1: 0.9966 - val_loss: 1.5485 - val_f1: 0.6954\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0063 - f1: 0.9971 - val_loss: 1.5327 - val_f1: 0.6835\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0134 - f1: 0.9948 - val_loss: 1.4766 - val_f1: 0.6836\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0083 - f1: 0.9955 - val_loss: 1.4799 - val_f1: 0.6899\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9985 - val_loss: 1.6058 - val_f1: 0.7017\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0042 - f1: 0.9976 - val_loss: 1.6331 - val_f1: 0.6840\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0108 - f1: 0.9962 - val_loss: 1.5800 - val_f1: 0.7007\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0106 - f1: 0.9960 - val_loss: 1.5320 - val_f1: 0.6861\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9978 - val_loss: 1.5270 - val_f1: 0.6904\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0127 - f1: 0.9948 - val_loss: 1.5452 - val_f1: 0.6979\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0050 - f1: 0.9983 - val_loss: 1.5282 - val_f1: 0.6948\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0032 - f1: 0.9991 - val_loss: 1.5470 - val_f1: 0.6947\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0028 - f1: 0.9981 - val_loss: 1.6314 - val_f1: 0.6852\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0039 - f1: 0.9981 - val_loss: 1.6364 - val_f1: 0.6991\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0069 - f1: 0.9972 - val_loss: 1.6108 - val_f1: 0.6866\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0091 - f1: 0.9964 - val_loss: 1.6259 - val_f1: 0.7104\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0276 - f1: 0.9869 - val_loss: 1.4566 - val_f1: 0.6788\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0190 - f1: 0.9905 - val_loss: 1.3236 - val_f1: 0.6917\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0133 - f1: 0.9931 - val_loss: 1.3630 - val_f1: 0.6873\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0042 - f1: 0.9983 - val_loss: 1.4996 - val_f1: 0.6824\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0068 - f1: 0.9967 - val_loss: 1.4906 - val_f1: 0.6793\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0060 - f1: 0.9978 - val_loss: 1.4941 - val_f1: 0.6796\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0033 - f1: 0.9983 - val_loss: 1.5693 - val_f1: 0.6897\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0040 - f1: 0.9978 - val_loss: 1.6065 - val_f1: 0.6811\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0016 - f1: 0.9995 - val_loss: 1.7039 - val_f1: 0.6915\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0018 - f1: 0.9997 - val_loss: 1.6946 - val_f1: 0.6765\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0015 - f1: 0.9997 - val_loss: 1.7161 - val_f1: 0.6900\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0037 - f1: 0.9990 - val_loss: 1.6921 - val_f1: 0.6781\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0034 - f1: 0.9981 - val_loss: 1.7299 - val_f1: 0.6838\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0029 - f1: 0.9988 - val_loss: 1.7502 - val_f1: 0.6878\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0065 - f1: 0.9973 - val_loss: 1.7642 - val_f1: 0.6943\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0125 - f1: 0.9947 - val_loss: 1.6837 - val_f1: 0.6686\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0128 - f1: 0.9936 - val_loss: 1.6332 - val_f1: 0.7029\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0061 - f1: 0.9969 - val_loss: 1.6569 - val_f1: 0.6804\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0023 - f1: 0.9995 - val_loss: 1.7264 - val_f1: 0.6915\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0027 - f1: 0.9988 - val_loss: 1.7220 - val_f1: 0.6711\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0036 - f1: 0.9986 - val_loss: 1.7102 - val_f1: 0.6777\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0030 - f1: 0.9983 - val_loss: 1.7227 - val_f1: 0.6921\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0020 - f1: 0.9991 - val_loss: 1.6778 - val_f1: 0.6941\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0020 - f1: 0.9988 - val_loss: 1.7141 - val_f1: 0.6955\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0034 - f1: 0.9990 - val_loss: 1.7164 - val_f1: 0.6817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0047 - f1: 0.9985 - val_loss: 1.6890 - val_f1: 0.6922\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0141 - f1: 0.9948 - val_loss: 1.6110 - val_f1: 0.6893\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0327 - f1: 0.9835 - val_loss: 1.2895 - val_f1: 0.6588\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0194 - f1: 0.9924 - val_loss: 1.3299 - val_f1: 0.6962\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0049 - f1: 0.9981 - val_loss: 1.4943 - val_f1: 0.7051\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9985 - val_loss: 1.5747 - val_f1: 0.6899\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0032 - f1: 0.9985 - val_loss: 1.6362 - val_f1: 0.6879\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0026 - f1: 0.9985 - val_loss: 1.6510 - val_f1: 0.6912\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0015 - f1: 0.9991 - val_loss: 1.6918 - val_f1: 0.6930\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0022 - f1: 0.9991 - val_loss: 1.6883 - val_f1: 0.6820\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0043 - f1: 0.9983 - val_loss: 1.7220 - val_f1: 0.6822\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0095 - f1: 0.9969 - val_loss: 1.6150 - val_f1: 0.7048\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0077 - f1: 0.9966 - val_loss: 1.6000 - val_f1: 0.6987\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0028 - f1: 0.9985 - val_loss: 1.6660 - val_f1: 0.6983\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0038 - f1: 0.9986 - val_loss: 1.7503 - val_f1: 0.6897\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0031 - f1: 0.9983 - val_loss: 1.8304 - val_f1: 0.7035\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0126 - f1: 0.9960 - val_loss: 1.6863 - val_f1: 0.6921\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0138 - f1: 0.9943 - val_loss: 1.4997 - val_f1: 0.6951\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0062 - f1: 0.9964 - val_loss: 1.5304 - val_f1: 0.6893\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0050 - f1: 0.9974 - val_loss: 1.5765 - val_f1: 0.6936\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0031 - f1: 0.9981 - val_loss: 1.6269 - val_f1: 0.7035\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0025 - f1: 0.9985 - val_loss: 1.7064 - val_f1: 0.7094\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0033 - f1: 0.9978 - val_loss: 1.6479 - val_f1: 0.6862\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0013 - f1: 0.9993 - val_loss: 1.6977 - val_f1: 0.6943\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0016 - f1: 0.9991 - val_loss: 1.7201 - val_f1: 0.7142\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0026 - f1: 0.9995 - val_loss: 1.7386 - val_f1: 0.6949\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0010 - f1: 0.9998 - val_loss: 1.8168 - val_f1: 0.7134\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0087 - f1: 0.9966 - val_loss: 1.7384 - val_f1: 0.6936\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0149 - f1: 0.9935 - val_loss: 1.4699 - val_f1: 0.6961\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0115 - f1: 0.9941 - val_loss: 1.5335 - val_f1: 0.6822\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0078 - f1: 0.9964 - val_loss: 1.5191 - val_f1: 0.7063\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0058 - f1: 0.9974 - val_loss: 1.5657 - val_f1: 0.6754\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0018 - f1: 0.9990 - val_loss: 1.6678 - val_f1: 0.6855\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0021 - f1: 0.9991 - val_loss: 1.7360 - val_f1: 0.6942\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0012 - f1: 0.9995 - val_loss: 1.7281 - val_f1: 0.6873\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0025 - f1: 0.9991 - val_loss: 1.8059 - val_f1: 0.6785\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0040 - f1: 0.9979 - val_loss: 1.7851 - val_f1: 0.6939\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0029 - f1: 0.9981 - val_loss: 1.7872 - val_f1: 0.6927\n",
      "213/213 [==============================] - 0s 329us/step\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      " 320/2906 [==>...........................] - ETA: 25s - loss: 0.6078 - f1: 0.4886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tharindu/anaconda3/envs/sentence_similarity_3.6/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.392454). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2906/2906 [==============================] - 7s 2ms/step - loss: 0.5313 - f1: 0.5909 - val_loss: 0.4918 - val_f1: 0.6245\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.4753 - f1: 0.6117 - val_loss: 0.4363 - val_f1: 0.6289\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.4318 - f1: 0.6878 - val_loss: 0.4192 - val_f1: 0.6907\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.4159 - f1: 0.7050 - val_loss: 0.4012 - val_f1: 0.7246\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3897 - f1: 0.7352 - val_loss: 0.4020 - val_f1: 0.7180\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3817 - f1: 0.7420 - val_loss: 0.3920 - val_f1: 0.7264\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 4s 1ms/step - loss: 0.3560 - f1: 0.7696 - val_loss: 0.3975 - val_f1: 0.7273\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3359 - f1: 0.7742 - val_loss: 0.4150 - val_f1: 0.7330\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.3201 - f1: 0.7926 - val_loss: 0.4114 - val_f1: 0.7139\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2981 - f1: 0.8059 - val_loss: 0.4288 - val_f1: 0.7164\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2724 - f1: 0.8217 - val_loss: 0.4524 - val_f1: 0.7062\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2610 - f1: 0.8325 - val_loss: 0.4723 - val_f1: 0.6913\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2313 - f1: 0.8544 - val_loss: 0.5705 - val_f1: 0.7144\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.2168 - f1: 0.8646 - val_loss: 0.5310 - val_f1: 0.7102\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1914 - f1: 0.8843 - val_loss: 0.6523 - val_f1: 0.6573\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.1840 - f1: 0.8862 - val_loss: 0.6016 - val_f1: 0.6912\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 4s 1ms/step - loss: 0.1609 - f1: 0.9021 - val_loss: 0.6341 - val_f1: 0.7016\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 4s 1ms/step - loss: 0.1227 - f1: 0.9302 - val_loss: 0.7286 - val_f1: 0.7097\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 4s 1ms/step - loss: 0.1137 - f1: 0.9353 - val_loss: 0.7761 - val_f1: 0.6714\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0882 - f1: 0.9533 - val_loss: 0.8430 - val_f1: 0.6837\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 995us/step - loss: 0.0810 - f1: 0.9554 - val_loss: 0.8639 - val_f1: 0.6577\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0718 - f1: 0.9598 - val_loss: 0.9018 - val_f1: 0.7044\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0649 - f1: 0.9661 - val_loss: 0.9405 - val_f1: 0.6890\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0483 - f1: 0.9765 - val_loss: 0.9807 - val_f1: 0.6835\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0452 - f1: 0.9795 - val_loss: 1.0208 - val_f1: 0.6735\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0524 - f1: 0.9752 - val_loss: 0.9913 - val_f1: 0.6556\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0553 - f1: 0.9707 - val_loss: 1.0175 - val_f1: 0.6965\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0420 - f1: 0.9781 - val_loss: 1.0800 - val_f1: 0.6689\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0371 - f1: 0.9803 - val_loss: 1.1065 - val_f1: 0.7074\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0751 - f1: 0.9643 - val_loss: 1.0136 - val_f1: 0.7144\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0553 - f1: 0.9686 - val_loss: 0.9799 - val_f1: 0.6879\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0375 - f1: 0.9796 - val_loss: 1.0192 - val_f1: 0.6888\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0228 - f1: 0.9887 - val_loss: 1.1323 - val_f1: 0.6959\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0202 - f1: 0.9895 - val_loss: 1.2449 - val_f1: 0.6718\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0189 - f1: 0.9914 - val_loss: 1.2105 - val_f1: 0.6828\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0148 - f1: 0.9945 - val_loss: 1.2668 - val_f1: 0.6880\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0127 - f1: 0.9952 - val_loss: 1.2843 - val_f1: 0.6892\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0237 - f1: 0.9888 - val_loss: 1.2752 - val_f1: 0.7009\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0237 - f1: 0.9873 - val_loss: 1.2392 - val_f1: 0.6823\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0263 - f1: 0.9881 - val_loss: 1.2707 - val_f1: 0.6904\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0218 - f1: 0.9917 - val_loss: 1.3023 - val_f1: 0.6811\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0361 - f1: 0.9825 - val_loss: 1.1245 - val_f1: 0.6389\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0245 - f1: 0.9899 - val_loss: 1.1509 - val_f1: 0.6771\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0274 - f1: 0.9866 - val_loss: 1.2740 - val_f1: 0.7000\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0171 - f1: 0.9911 - val_loss: 1.2585 - val_f1: 0.6932\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0113 - f1: 0.9945 - val_loss: 1.3133 - val_f1: 0.6919\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0121 - f1: 0.9947 - val_loss: 1.3952 - val_f1: 0.6955\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0088 - f1: 0.9960 - val_loss: 1.3951 - val_f1: 0.6940\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0106 - f1: 0.9955 - val_loss: 1.3626 - val_f1: 0.6686\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0138 - f1: 0.9940 - val_loss: 1.4090 - val_f1: 0.6922\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0156 - f1: 0.9929 - val_loss: 1.4083 - val_f1: 0.7037\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0104 - f1: 0.9957 - val_loss: 1.4180 - val_f1: 0.6903\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0184 - f1: 0.9921 - val_loss: 1.4051 - val_f1: 0.6656\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0168 - f1: 0.9940 - val_loss: 1.3363 - val_f1: 0.7032\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0236 - f1: 0.9899 - val_loss: 1.5213 - val_f1: 0.7000\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0269 - f1: 0.9854 - val_loss: 1.2028 - val_f1: 0.7005\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0153 - f1: 0.9931 - val_loss: 1.3889 - val_f1: 0.6928\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0109 - f1: 0.9950 - val_loss: 1.3459 - val_f1: 0.6796\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0072 - f1: 0.9966 - val_loss: 1.5005 - val_f1: 0.6825\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0131 - f1: 0.9952 - val_loss: 1.4621 - val_f1: 0.7012\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0075 - f1: 0.9962 - val_loss: 1.4168 - val_f1: 0.6882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0059 - f1: 0.9986 - val_loss: 1.4634 - val_f1: 0.6895\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0060 - f1: 0.9979 - val_loss: 1.4786 - val_f1: 0.6839\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0068 - f1: 0.9969 - val_loss: 1.6151 - val_f1: 0.6911\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0112 - f1: 0.9960 - val_loss: 1.5387 - val_f1: 0.6921\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0178 - f1: 0.9928 - val_loss: 1.3922 - val_f1: 0.6829\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0129 - f1: 0.9953 - val_loss: 1.3869 - val_f1: 0.6935\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0063 - f1: 0.9969 - val_loss: 1.4835 - val_f1: 0.6959\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0067 - f1: 0.9979 - val_loss: 1.5161 - val_f1: 0.6991\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0098 - f1: 0.9959 - val_loss: 1.5306 - val_f1: 0.6822\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9959 - val_loss: 1.4756 - val_f1: 0.6919\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0105 - f1: 0.9955 - val_loss: 1.4316 - val_f1: 0.6981\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0059 - f1: 0.9988 - val_loss: 1.5359 - val_f1: 0.7060\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0067 - f1: 0.9966 - val_loss: 1.4599 - val_f1: 0.6964\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0076 - f1: 0.9959 - val_loss: 1.4784 - val_f1: 0.6829\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0105 - f1: 0.9966 - val_loss: 1.4720 - val_f1: 0.7084\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9976 - val_loss: 1.5251 - val_f1: 0.6925\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0093 - f1: 0.9953 - val_loss: 1.5170 - val_f1: 0.6945\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0063 - f1: 0.9972 - val_loss: 1.5023 - val_f1: 0.6908\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0149 - f1: 0.9933 - val_loss: 1.4736 - val_f1: 0.6825\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0227 - f1: 0.9898 - val_loss: 1.2579 - val_f1: 0.6802\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0160 - f1: 0.9921 - val_loss: 1.3408 - val_f1: 0.6897\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0202 - f1: 0.9902 - val_loss: 1.3470 - val_f1: 0.7025\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0170 - f1: 0.9914 - val_loss: 1.3059 - val_f1: 0.7162\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0040 - f1: 0.9993 - val_loss: 1.3996 - val_f1: 0.6970\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0051 - f1: 0.9974 - val_loss: 1.4398 - val_f1: 0.6993\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9983 - val_loss: 1.4721 - val_f1: 0.6995\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0054 - f1: 0.9972 - val_loss: 1.4995 - val_f1: 0.7090\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0022 - f1: 0.9997 - val_loss: 1.5138 - val_f1: 0.7074\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9991 - val_loss: 1.6198 - val_f1: 0.7155\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9990 - val_loss: 1.5991 - val_f1: 0.6900\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0070 - f1: 0.9981 - val_loss: 1.6280 - val_f1: 0.7045\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9974 - val_loss: 1.6511 - val_f1: 0.6931\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0066 - f1: 0.9962 - val_loss: 1.6596 - val_f1: 0.6966\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0165 - f1: 0.9926 - val_loss: 1.5100 - val_f1: 0.6795\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0104 - f1: 0.9952 - val_loss: 1.5548 - val_f1: 0.6956\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0105 - f1: 0.9955 - val_loss: 1.5749 - val_f1: 0.6815\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0081 - f1: 0.9976 - val_loss: 1.5738 - val_f1: 0.7058\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0047 - f1: 0.9981 - val_loss: 1.5709 - val_f1: 0.6816\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0074 - f1: 0.9967 - val_loss: 1.6184 - val_f1: 0.6633\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0114 - f1: 0.9959 - val_loss: 1.5620 - val_f1: 0.7024\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0060 - f1: 0.9976 - val_loss: 1.5363 - val_f1: 0.6891\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0036 - f1: 0.9981 - val_loss: 1.5682 - val_f1: 0.6741\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0075 - f1: 0.9967 - val_loss: 1.5443 - val_f1: 0.6810\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0036 - f1: 0.9983 - val_loss: 1.6242 - val_f1: 0.7014\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0085 - f1: 0.9969 - val_loss: 1.6234 - val_f1: 0.6805\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0094 - f1: 0.9967 - val_loss: 1.5381 - val_f1: 0.6898\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0066 - f1: 0.9972 - val_loss: 1.6150 - val_f1: 0.6983\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9971 - val_loss: 1.5828 - val_f1: 0.6987\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9979 - val_loss: 1.5600 - val_f1: 0.6835\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0051 - f1: 0.9976 - val_loss: 1.5656 - val_f1: 0.6752\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0052 - f1: 0.9971 - val_loss: 1.6380 - val_f1: 0.6849\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0018 - f1: 0.9997 - val_loss: 1.6953 - val_f1: 0.6815\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0020 - f1: 0.9993 - val_loss: 1.7347 - val_f1: 0.6890\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0036 - f1: 0.9990 - val_loss: 1.7169 - val_f1: 0.6801\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0087 - f1: 0.9966 - val_loss: 1.7054 - val_f1: 0.6893\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0154 - f1: 0.9938 - val_loss: 1.6109 - val_f1: 0.6835\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0076 - f1: 0.9964 - val_loss: 1.5158 - val_f1: 0.6904\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0079 - f1: 0.9969 - val_loss: 1.6363 - val_f1: 0.6910\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0097 - f1: 0.9941 - val_loss: 1.4955 - val_f1: 0.6773\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0061 - f1: 0.9966 - val_loss: 1.6115 - val_f1: 0.6746\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0079 - f1: 0.9962 - val_loss: 1.5916 - val_f1: 0.6856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0091 - f1: 0.9966 - val_loss: 1.5316 - val_f1: 0.7014\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0182 - f1: 0.9923 - val_loss: 1.4913 - val_f1: 0.7110\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0211 - f1: 0.9891 - val_loss: 1.4077 - val_f1: 0.6991\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9959 - val_loss: 1.4235 - val_f1: 0.7019\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9974 - val_loss: 1.4500 - val_f1: 0.6959\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0038 - f1: 0.9983 - val_loss: 1.4650 - val_f1: 0.6907\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0054 - f1: 0.9981 - val_loss: 1.5518 - val_f1: 0.6842\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0062 - f1: 0.9978 - val_loss: 1.5084 - val_f1: 0.6824\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0030 - f1: 0.9991 - val_loss: 1.4907 - val_f1: 0.6865\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0016 - f1: 0.9998 - val_loss: 1.5709 - val_f1: 0.6918\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0054 - f1: 0.9979 - val_loss: 1.6233 - val_f1: 0.6853\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0092 - f1: 0.9959 - val_loss: 1.5650 - val_f1: 0.6573\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0124 - f1: 0.9935 - val_loss: 1.5694 - val_f1: 0.6904\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0101 - f1: 0.9955 - val_loss: 1.4791 - val_f1: 0.6885\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9986 - val_loss: 1.4947 - val_f1: 0.6746\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0056 - f1: 0.9981 - val_loss: 1.5405 - val_f1: 0.6959\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0045 - f1: 0.9986 - val_loss: 1.5735 - val_f1: 0.7041\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0019 - f1: 0.9997 - val_loss: 1.5737 - val_f1: 0.6888\n",
      "Train on 2906 samples, validate on 970 samples\n",
      "Epoch 1/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0023 - f1: 0.9993 - val_loss: 1.5645 - val_f1: 0.6801\n",
      "Epoch 2/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0026 - f1: 0.9990 - val_loss: 1.5889 - val_f1: 0.6894\n",
      "Epoch 3/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0045 - f1: 0.9986 - val_loss: 1.6089 - val_f1: 0.6757\n",
      "Epoch 4/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0043 - f1: 0.9978 - val_loss: 1.6598 - val_f1: 0.6877\n",
      "Epoch 5/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0054 - f1: 0.9983 - val_loss: 1.6437 - val_f1: 0.6738\n",
      "Epoch 6/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0081 - f1: 0.9967 - val_loss: 1.5462 - val_f1: 0.6983\n",
      "Epoch 7/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9985 - val_loss: 1.6645 - val_f1: 0.6880\n",
      "Epoch 8/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0059 - f1: 0.9972 - val_loss: 1.5621 - val_f1: 0.6938\n",
      "Epoch 9/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0075 - f1: 0.9957 - val_loss: 1.5513 - val_f1: 0.6918\n",
      "Epoch 10/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0031 - f1: 0.9988 - val_loss: 1.6115 - val_f1: 0.6960\n",
      "Epoch 11/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0033 - f1: 0.9985 - val_loss: 1.7139 - val_f1: 0.7104\n",
      "Epoch 12/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0053 - f1: 0.9978 - val_loss: 1.5964 - val_f1: 0.6791\n",
      "Epoch 13/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0020 - f1: 0.9991 - val_loss: 1.7078 - val_f1: 0.7019\n",
      "Epoch 14/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0024 - f1: 0.9993 - val_loss: 1.8325 - val_f1: 0.7083\n",
      "Epoch 15/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9988 - val_loss: 1.6684 - val_f1: 0.6814\n",
      "Epoch 16/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0037 - f1: 0.9991 - val_loss: 1.6262 - val_f1: 0.6953\n",
      "Epoch 17/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0025 - f1: 0.9991 - val_loss: 1.7091 - val_f1: 0.7054\n",
      "Epoch 18/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0048 - f1: 0.9978 - val_loss: 1.6613 - val_f1: 0.6898\n",
      "Epoch 19/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0017 - f1: 0.9993 - val_loss: 1.7257 - val_f1: 0.7021\n",
      "Epoch 20/20\n",
      "2906/2906 [==============================] - 3s 1ms/step - loss: 0.0018 - f1: 0.9986 - val_loss: 1.7578 - val_f1: 0.7005\n",
      "213/213 [==============================] - 0s 370us/step\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      " 320/2907 [==>...........................] - ETA: 26s - loss: 0.5899 - f1: 0.5612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tharindu/anaconda3/envs/sentence_similarity_3.6/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.422983). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2907/2907 [==============================] - 7s 2ms/step - loss: 0.5149 - f1: 0.6088 - val_loss: 0.4450 - val_f1: 0.6821\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.4417 - f1: 0.6770 - val_loss: 0.4192 - val_f1: 0.7036\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.4201 - f1: 0.7054 - val_loss: 0.4078 - val_f1: 0.7148\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.4062 - f1: 0.7170 - val_loss: 0.4086 - val_f1: 0.7194\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3937 - f1: 0.7284 - val_loss: 0.4011 - val_f1: 0.7201\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3790 - f1: 0.7427 - val_loss: 0.3954 - val_f1: 0.7302\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3725 - f1: 0.7461 - val_loss: 0.4165 - val_f1: 0.7259\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3539 - f1: 0.7629 - val_loss: 0.4224 - val_f1: 0.7238\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3389 - f1: 0.7780 - val_loss: 0.4116 - val_f1: 0.7085\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3237 - f1: 0.7886 - val_loss: 0.4400 - val_f1: 0.7241\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.3018 - f1: 0.8008 - val_loss: 0.4322 - val_f1: 0.7147\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.2761 - f1: 0.8206 - val_loss: 0.4673 - val_f1: 0.6871\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.2616 - f1: 0.8289 - val_loss: 0.4710 - val_f1: 0.7181\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.2275 - f1: 0.8557 - val_loss: 0.5310 - val_f1: 0.6992\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.2060 - f1: 0.8714 - val_loss: 0.6292 - val_f1: 0.6863\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.1722 - f1: 0.8958 - val_loss: 0.5950 - val_f1: 0.6777\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.1482 - f1: 0.9133 - val_loss: 0.6060 - val_f1: 0.6833\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.1208 - f1: 0.9272 - val_loss: 0.7091 - val_f1: 0.6940\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.1113 - f1: 0.9314 - val_loss: 0.7248 - val_f1: 0.6578\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0978 - f1: 0.9458 - val_loss: 0.6999 - val_f1: 0.6495\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0984 - f1: 0.9438 - val_loss: 0.8356 - val_f1: 0.6993\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0753 - f1: 0.9558 - val_loss: 0.8807 - val_f1: 0.6764\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0529 - f1: 0.9701 - val_loss: 0.9063 - val_f1: 0.6932\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0694 - f1: 0.9599 - val_loss: 0.9341 - val_f1: 0.6847\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0587 - f1: 0.9669 - val_loss: 0.9865 - val_f1: 0.6884\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0539 - f1: 0.9720 - val_loss: 0.9506 - val_f1: 0.6553\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0649 - f1: 0.9668 - val_loss: 0.8995 - val_f1: 0.6878\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0394 - f1: 0.9819 - val_loss: 1.0133 - val_f1: 0.6847\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0348 - f1: 0.9829 - val_loss: 1.0387 - val_f1: 0.6696\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0287 - f1: 0.9855 - val_loss: 1.1197 - val_f1: 0.6755\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0346 - f1: 0.9820 - val_loss: 1.0945 - val_f1: 0.6955\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0287 - f1: 0.9852 - val_loss: 1.2474 - val_f1: 0.6709\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0195 - f1: 0.9912 - val_loss: 1.2401 - val_f1: 0.6761\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0161 - f1: 0.9928 - val_loss: 1.2099 - val_f1: 0.6781\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0200 - f1: 0.9916 - val_loss: 1.2787 - val_f1: 0.6770\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0174 - f1: 0.9909 - val_loss: 1.2487 - val_f1: 0.6823\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0173 - f1: 0.9917 - val_loss: 1.3979 - val_f1: 0.6909\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0365 - f1: 0.9822 - val_loss: 1.1681 - val_f1: 0.6781\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0181 - f1: 0.9927 - val_loss: 1.1747 - val_f1: 0.6895\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0171 - f1: 0.9924 - val_loss: 1.2239 - val_f1: 0.6814\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0170 - f1: 0.9923 - val_loss: 1.2652 - val_f1: 0.6998\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0210 - f1: 0.9904 - val_loss: 1.3194 - val_f1: 0.6830\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0162 - f1: 0.9933 - val_loss: 1.2946 - val_f1: 0.6736\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0121 - f1: 0.9941 - val_loss: 1.2887 - val_f1: 0.6879\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0160 - f1: 0.9947 - val_loss: 1.2662 - val_f1: 0.6805\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0210 - f1: 0.9909 - val_loss: 1.1837 - val_f1: 0.6880\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0110 - f1: 0.9952 - val_loss: 1.2879 - val_f1: 0.6789\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0237 - f1: 0.9897 - val_loss: 1.2613 - val_f1: 0.6878\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0180 - f1: 0.9916 - val_loss: 1.3297 - val_f1: 0.6832\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0103 - f1: 0.9952 - val_loss: 1.3709 - val_f1: 0.6930\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0090 - f1: 0.9966 - val_loss: 1.3804 - val_f1: 0.6885\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0154 - f1: 0.9931 - val_loss: 1.3857 - val_f1: 0.6871\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0101 - f1: 0.9960 - val_loss: 1.3413 - val_f1: 0.6966\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0065 - f1: 0.9971 - val_loss: 1.3832 - val_f1: 0.6890\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9979 - val_loss: 1.5077 - val_f1: 0.6703\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0034 - f1: 0.9988 - val_loss: 1.5129 - val_f1: 0.6809\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0024 - f1: 0.9995 - val_loss: 1.5385 - val_f1: 0.6787\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0033 - f1: 0.9985 - val_loss: 1.5807 - val_f1: 0.6806\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0074 - f1: 0.9969 - val_loss: 1.6040 - val_f1: 0.6704\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0086 - f1: 0.9959 - val_loss: 1.5078 - val_f1: 0.6891\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0065 - f1: 0.9964 - val_loss: 1.5347 - val_f1: 0.6977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0029 - f1: 0.9990 - val_loss: 1.5323 - val_f1: 0.6935\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 4s 1ms/step - loss: 0.0032 - f1: 0.9990 - val_loss: 1.5607 - val_f1: 0.6965\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 4s 1ms/step - loss: 0.0050 - f1: 0.9985 - val_loss: 1.6361 - val_f1: 0.6912\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 4s 1ms/step - loss: 0.0152 - f1: 0.9905 - val_loss: 1.6408 - val_f1: 0.6836\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0397 - f1: 0.9803 - val_loss: 1.2513 - val_f1: 0.6911\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0151 - f1: 0.9919 - val_loss: 1.3233 - val_f1: 0.6896\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0075 - f1: 0.9959 - val_loss: 1.3635 - val_f1: 0.6820\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0067 - f1: 0.9969 - val_loss: 1.4398 - val_f1: 0.6946\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0115 - f1: 0.9957 - val_loss: 1.4974 - val_f1: 0.6819\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0072 - f1: 0.9976 - val_loss: 1.4426 - val_f1: 0.6922\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0028 - f1: 0.9988 - val_loss: 1.4620 - val_f1: 0.6867\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0083 - f1: 0.9962 - val_loss: 1.4559 - val_f1: 0.6850\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0061 - f1: 0.9967 - val_loss: 1.4576 - val_f1: 0.6688\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0048 - f1: 0.9985 - val_loss: 1.6395 - val_f1: 0.6879\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0069 - f1: 0.9964 - val_loss: 1.4863 - val_f1: 0.6683\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0049 - f1: 0.9976 - val_loss: 1.5142 - val_f1: 0.6823\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0052 - f1: 0.9973 - val_loss: 1.5260 - val_f1: 0.6949\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0058 - f1: 0.9967 - val_loss: 1.6393 - val_f1: 0.7003\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0136 - f1: 0.9942 - val_loss: 1.4744 - val_f1: 0.6702\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0132 - f1: 0.9929 - val_loss: 1.4127 - val_f1: 0.6884\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0060 - f1: 0.9974 - val_loss: 1.5007 - val_f1: 0.6884\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0153 - f1: 0.9947 - val_loss: 1.3595 - val_f1: 0.6870\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0027 - f1: 0.9993 - val_loss: 1.3685 - val_f1: 0.6987\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0071 - f1: 0.9972 - val_loss: 1.3894 - val_f1: 0.6745\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0022 - f1: 0.9993 - val_loss: 1.4919 - val_f1: 0.6832\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0013 - f1: 0.9998 - val_loss: 1.5298 - val_f1: 0.6796\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0020 - f1: 0.9990 - val_loss: 1.5990 - val_f1: 0.6926\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0017 - f1: 0.9995 - val_loss: 1.6289 - val_f1: 0.6907\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0023 - f1: 0.9990 - val_loss: 1.7938 - val_f1: 0.6811\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0018 - f1: 0.9991 - val_loss: 1.7315 - val_f1: 0.6953\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9983 - val_loss: 1.6896 - val_f1: 0.6933\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0084 - f1: 0.9966 - val_loss: 1.6680 - val_f1: 0.7043\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0155 - f1: 0.9924 - val_loss: 1.5086 - val_f1: 0.6806\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0139 - f1: 0.9935 - val_loss: 1.5063 - val_f1: 0.6867\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0148 - f1: 0.9909 - val_loss: 1.3351 - val_f1: 0.6927\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0084 - f1: 0.9954 - val_loss: 1.4954 - val_f1: 0.7047\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0119 - f1: 0.9943 - val_loss: 1.4162 - val_f1: 0.6899\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0042 - f1: 0.9986 - val_loss: 1.4596 - val_f1: 0.7005\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0050 - f1: 0.9986 - val_loss: 1.5067 - val_f1: 0.6988\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0082 - f1: 0.9966 - val_loss: 1.5706 - val_f1: 0.6897\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0050 - f1: 0.9974 - val_loss: 1.4605 - val_f1: 0.6849\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0036 - f1: 0.9981 - val_loss: 1.5030 - val_f1: 0.6899\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0029 - f1: 0.9988 - val_loss: 1.5207 - val_f1: 0.6939\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0034 - f1: 0.9988 - val_loss: 1.5760 - val_f1: 0.6930\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0065 - f1: 0.9971 - val_loss: 1.5377 - val_f1: 0.7051\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0028 - f1: 0.9983 - val_loss: 1.5545 - val_f1: 0.6886\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0070 - f1: 0.9960 - val_loss: 1.4547 - val_f1: 0.6895\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0045 - f1: 0.9983 - val_loss: 1.4853 - val_f1: 0.7131\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0070 - f1: 0.9974 - val_loss: 1.4412 - val_f1: 0.7133\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0067 - f1: 0.9971 - val_loss: 1.4043 - val_f1: 0.6892\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0081 - f1: 0.9964 - val_loss: 1.4100 - val_f1: 0.6855\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0049 - f1: 0.9973 - val_loss: 1.5073 - val_f1: 0.6984\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0039 - f1: 0.9981 - val_loss: 1.6468 - val_f1: 0.6822\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0171 - f1: 0.9931 - val_loss: 1.4784 - val_f1: 0.6729\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0072 - f1: 0.9969 - val_loss: 1.3830 - val_f1: 0.6709\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0096 - f1: 0.9943 - val_loss: 1.3598 - val_f1: 0.6884\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0144 - f1: 0.9926 - val_loss: 1.4855 - val_f1: 0.6983\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0098 - f1: 0.9941 - val_loss: 1.5203 - val_f1: 0.7062\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0045 - f1: 0.9985 - val_loss: 1.4541 - val_f1: 0.6932\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9978 - val_loss: 1.4961 - val_f1: 0.6975\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0013 - f1: 0.9995 - val_loss: 1.5865 - val_f1: 0.6968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0014 - f1: 0.9995 - val_loss: 1.6042 - val_f1: 0.6942\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0014 - f1: 0.9997 - val_loss: 1.5788 - val_f1: 0.6945\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0011 - f1: 0.9995 - val_loss: 1.6197 - val_f1: 0.7031\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0014 - f1: 0.9997 - val_loss: 1.6555 - val_f1: 0.6978\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0015 - f1: 0.9997 - val_loss: 1.6601 - val_f1: 0.6883\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0104 - f1: 0.9954 - val_loss: 1.5182 - val_f1: 0.6917\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0051 - f1: 0.9979 - val_loss: 1.6636 - val_f1: 0.6961\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0062 - f1: 0.9966 - val_loss: 1.6846 - val_f1: 0.6798\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0094 - f1: 0.9972 - val_loss: 1.5443 - val_f1: 0.6880\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0058 - f1: 0.9979 - val_loss: 1.5644 - val_f1: 0.6913\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0033 - f1: 0.9986 - val_loss: 1.5929 - val_f1: 0.6732\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0013 - f1: 0.9995 - val_loss: 1.6582 - val_f1: 0.6854\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 7.1111e-04 - f1: 0.9998 - val_loss: 1.6565 - val_f1: 0.6770\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0027 - f1: 0.9988 - val_loss: 1.6755 - val_f1: 0.6794\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0032 - f1: 0.9990 - val_loss: 1.6329 - val_f1: 0.6851\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0017 - f1: 0.9995 - val_loss: 1.7071 - val_f1: 0.6918\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 7.8493e-04 - f1: 1.0000 - val_loss: 1.7037 - val_f1: 0.6864\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0012 - f1: 0.9997 - val_loss: 1.7420 - val_f1: 0.6903\n",
      "Train on 2907 samples, validate on 969 samples\n",
      "Epoch 1/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 5.6308e-04 - f1: 0.9998 - val_loss: 1.7526 - val_f1: 0.6895\n",
      "Epoch 2/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 8.8347e-04 - f1: 0.9993 - val_loss: 1.8078 - val_f1: 0.7035\n",
      "Epoch 3/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0022 - f1: 0.9988 - val_loss: 1.9244 - val_f1: 0.6935\n",
      "Epoch 4/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0076 - f1: 0.9962 - val_loss: 1.5959 - val_f1: 0.6816\n",
      "Epoch 5/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0052 - f1: 0.9979 - val_loss: 1.6687 - val_f1: 0.6877\n",
      "Epoch 6/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0045 - f1: 0.9966 - val_loss: 1.8114 - val_f1: 0.7000\n",
      "Epoch 7/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0298 - f1: 0.9880 - val_loss: 1.3515 - val_f1: 0.6956\n",
      "Epoch 8/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0053 - f1: 0.9981 - val_loss: 1.3925 - val_f1: 0.6832\n",
      "Epoch 9/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9988 - val_loss: 1.4663 - val_f1: 0.7038\n",
      "Epoch 10/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0053 - f1: 0.9981 - val_loss: 1.4554 - val_f1: 0.7014\n",
      "Epoch 11/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0019 - f1: 0.9993 - val_loss: 1.5216 - val_f1: 0.6993\n",
      "Epoch 12/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0037 - f1: 0.9978 - val_loss: 1.5255 - val_f1: 0.7082\n",
      "Epoch 13/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9978 - val_loss: 1.4774 - val_f1: 0.6945\n",
      "Epoch 14/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0033 - f1: 0.9986 - val_loss: 1.4785 - val_f1: 0.6864\n",
      "Epoch 15/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0058 - f1: 0.9966 - val_loss: 1.6153 - val_f1: 0.7063\n",
      "Epoch 16/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0069 - f1: 0.9966 - val_loss: 1.6002 - val_f1: 0.6922\n",
      "Epoch 17/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0084 - f1: 0.9960 - val_loss: 1.5703 - val_f1: 0.6784\n",
      "Epoch 18/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0040 - f1: 0.9985 - val_loss: 1.6175 - val_f1: 0.6872\n",
      "Epoch 19/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0016 - f1: 0.9993 - val_loss: 1.6638 - val_f1: 0.6907\n",
      "Epoch 20/20\n",
      "2907/2907 [==============================] - 3s 1ms/step - loss: 0.0022 - f1: 0.9991 - val_loss: 1.7504 - val_f1: 0.6877\n",
      "213/213 [==============================] - 0s 347us/step\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      " 320/2909 [==>...........................] - ETA: 26s - loss: 0.6040 - f1: 0.6066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tharindu/anaconda3/envs/sentence_similarity_3.6/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.434663). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2909/2909 [==============================] - 7s 2ms/step - loss: 0.5469 - f1: 0.5859 - val_loss: 0.4935 - val_f1: 0.6312\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.4835 - f1: 0.6422 - val_loss: 0.4390 - val_f1: 0.6925\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.4431 - f1: 0.6884 - val_loss: 0.4348 - val_f1: 0.6950\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.4307 - f1: 0.6991 - val_loss: 0.4294 - val_f1: 0.6993\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.4109 - f1: 0.7220 - val_loss: 0.4272 - val_f1: 0.6956\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.3955 - f1: 0.7364 - val_loss: 0.4276 - val_f1: 0.7006\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.3860 - f1: 0.7440 - val_loss: 0.4213 - val_f1: 0.7074\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.3801 - f1: 0.7441 - val_loss: 0.4301 - val_f1: 0.6815\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.3628 - f1: 0.7591 - val_loss: 0.4234 - val_f1: 0.6930\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.3442 - f1: 0.7809 - val_loss: 0.4712 - val_f1: 0.6901\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.3216 - f1: 0.8002 - val_loss: 0.4594 - val_f1: 0.6998\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.2983 - f1: 0.8141 - val_loss: 0.4514 - val_f1: 0.6962\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.2789 - f1: 0.8325 - val_loss: 0.4991 - val_f1: 0.6918\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.2493 - f1: 0.8563 - val_loss: 0.5311 - val_f1: 0.6893\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.2330 - f1: 0.8632 - val_loss: 0.5475 - val_f1: 0.6913\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.2173 - f1: 0.8746 - val_loss: 0.5929 - val_f1: 0.6943\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.1882 - f1: 0.8930 - val_loss: 0.5631 - val_f1: 0.6871\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.1686 - f1: 0.9058 - val_loss: 0.5973 - val_f1: 0.6862\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.1618 - f1: 0.9069 - val_loss: 0.6123 - val_f1: 0.6664\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.1385 - f1: 0.9266 - val_loss: 0.6912 - val_f1: 0.6756\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.1238 - f1: 0.9356 - val_loss: 0.6976 - val_f1: 0.6854\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.1193 - f1: 0.9404 - val_loss: 0.7053 - val_f1: 0.6962\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0870 - f1: 0.9583 - val_loss: 0.8343 - val_f1: 0.6994\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0895 - f1: 0.9551 - val_loss: 0.8236 - val_f1: 0.6909\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0781 - f1: 0.9615 - val_loss: 0.8885 - val_f1: 0.6707\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0672 - f1: 0.9651 - val_loss: 0.8795 - val_f1: 0.6560\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0589 - f1: 0.9689 - val_loss: 0.9180 - val_f1: 0.6677\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0723 - f1: 0.9641 - val_loss: 0.8566 - val_f1: 0.6814\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0470 - f1: 0.9792 - val_loss: 0.9479 - val_f1: 0.6887\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0373 - f1: 0.9835 - val_loss: 1.0286 - val_f1: 0.6854\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0285 - f1: 0.9868 - val_loss: 1.1034 - val_f1: 0.6622\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0417 - f1: 0.9817 - val_loss: 1.1271 - val_f1: 0.6934\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0430 - f1: 0.9795 - val_loss: 1.0060 - val_f1: 0.6797\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0435 - f1: 0.9800 - val_loss: 0.9902 - val_f1: 0.7056\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0357 - f1: 0.9836 - val_loss: 1.0572 - val_f1: 0.6883\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0359 - f1: 0.9826 - val_loss: 1.0757 - val_f1: 0.7075\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0269 - f1: 0.9883 - val_loss: 1.1048 - val_f1: 0.6779\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0369 - f1: 0.9833 - val_loss: 1.0655 - val_f1: 0.6875\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0264 - f1: 0.9870 - val_loss: 1.1175 - val_f1: 0.6842\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0169 - f1: 0.9928 - val_loss: 1.3389 - val_f1: 0.6999\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0134 - f1: 0.9943 - val_loss: 1.2707 - val_f1: 0.6800\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0144 - f1: 0.9928 - val_loss: 1.2120 - val_f1: 0.6657\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0200 - f1: 0.9918 - val_loss: 1.2416 - val_f1: 0.7017\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0139 - f1: 0.9942 - val_loss: 1.3732 - val_f1: 0.7024\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0235 - f1: 0.9878 - val_loss: 1.2903 - val_f1: 0.6806\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0289 - f1: 0.9866 - val_loss: 1.1862 - val_f1: 0.7022\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0133 - f1: 0.9941 - val_loss: 1.2541 - val_f1: 0.7007\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0238 - f1: 0.9881 - val_loss: 1.2080 - val_f1: 0.6857\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0189 - f1: 0.9916 - val_loss: 1.2721 - val_f1: 0.6875\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0168 - f1: 0.9933 - val_loss: 1.2313 - val_f1: 0.6858\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0150 - f1: 0.9938 - val_loss: 1.2798 - val_f1: 0.6989\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0135 - f1: 0.9933 - val_loss: 1.3621 - val_f1: 0.6801\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0096 - f1: 0.9950 - val_loss: 1.3825 - val_f1: 0.6718\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0139 - f1: 0.9952 - val_loss: 1.4143 - val_f1: 0.7043\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0096 - f1: 0.9954 - val_loss: 1.3709 - val_f1: 0.6878\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0139 - f1: 0.9923 - val_loss: 1.3381 - val_f1: 0.6888\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0115 - f1: 0.9955 - val_loss: 1.4556 - val_f1: 0.7050\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0143 - f1: 0.9935 - val_loss: 1.3282 - val_f1: 0.6925\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0175 - f1: 0.9936 - val_loss: 1.3378 - val_f1: 0.7079\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0168 - f1: 0.9945 - val_loss: 1.2257 - val_f1: 0.6975\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0119 - f1: 0.9952 - val_loss: 1.3375 - val_f1: 0.6934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0111 - f1: 0.9947 - val_loss: 1.4016 - val_f1: 0.7082\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0135 - f1: 0.9954 - val_loss: 1.3413 - val_f1: 0.7048\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9961 - val_loss: 1.3631 - val_f1: 0.7112\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0048 - f1: 0.9981 - val_loss: 1.4887 - val_f1: 0.7118\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0098 - f1: 0.9967 - val_loss: 1.4342 - val_f1: 0.6801\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0113 - f1: 0.9945 - val_loss: 1.4243 - val_f1: 0.6898\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0065 - f1: 0.9976 - val_loss: 1.4237 - val_f1: 0.6841\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9971 - val_loss: 1.4615 - val_f1: 0.6861\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0045 - f1: 0.9981 - val_loss: 1.5952 - val_f1: 0.6858\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0102 - f1: 0.9943 - val_loss: 1.7009 - val_f1: 0.6989\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0262 - f1: 0.9876 - val_loss: 1.3817 - val_f1: 0.6954\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0168 - f1: 0.9923 - val_loss: 1.3644 - val_f1: 0.6823\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0157 - f1: 0.9919 - val_loss: 1.3368 - val_f1: 0.6741\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0105 - f1: 0.9943 - val_loss: 1.4260 - val_f1: 0.6911\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0085 - f1: 0.9966 - val_loss: 1.3851 - val_f1: 0.6834\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0031 - f1: 0.9990 - val_loss: 1.4999 - val_f1: 0.6862\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9978 - val_loss: 1.5450 - val_f1: 0.6954\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0056 - f1: 0.9983 - val_loss: 1.5922 - val_f1: 0.6829\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0037 - f1: 0.9986 - val_loss: 1.6139 - val_f1: 0.6951\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0109 - f1: 0.9952 - val_loss: 1.4991 - val_f1: 0.6948\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0105 - f1: 0.9957 - val_loss: 1.5598 - val_f1: 0.6878\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0074 - f1: 0.9964 - val_loss: 1.5850 - val_f1: 0.6997\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0075 - f1: 0.9976 - val_loss: 1.6005 - val_f1: 0.6985\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0091 - f1: 0.9969 - val_loss: 1.6070 - val_f1: 0.6869\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0062 - f1: 0.9967 - val_loss: 1.6111 - val_f1: 0.6892\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0024 - f1: 0.9991 - val_loss: 1.6192 - val_f1: 0.7012\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0042 - f1: 0.9980 - val_loss: 1.6465 - val_f1: 0.6957\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0108 - f1: 0.9959 - val_loss: 1.5757 - val_f1: 0.6968\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0243 - f1: 0.9881 - val_loss: 1.4673 - val_f1: 0.6784\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0169 - f1: 0.9914 - val_loss: 1.3437 - val_f1: 0.6902\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0070 - f1: 0.9971 - val_loss: 1.3995 - val_f1: 0.6879\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0107 - f1: 0.9948 - val_loss: 1.4582 - val_f1: 0.6981\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0042 - f1: 0.9971 - val_loss: 1.5468 - val_f1: 0.7014\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9991 - val_loss: 1.6083 - val_f1: 0.6805\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0031 - f1: 0.9991 - val_loss: 1.5446 - val_f1: 0.6712\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0017 - f1: 0.9995 - val_loss: 1.5989 - val_f1: 0.6858\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0023 - f1: 0.9986 - val_loss: 1.6593 - val_f1: 0.6831\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0102 - f1: 0.9959 - val_loss: 1.6378 - val_f1: 0.6928\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0186 - f1: 0.9914 - val_loss: 1.3992 - val_f1: 0.6895\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9955 - val_loss: 1.4422 - val_f1: 0.6947\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9983 - val_loss: 1.4792 - val_f1: 0.6882\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9988 - val_loss: 1.5751 - val_f1: 0.6888\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0064 - f1: 0.9969 - val_loss: 1.6691 - val_f1: 0.6937\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0074 - f1: 0.9971 - val_loss: 1.6290 - val_f1: 0.6945\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0092 - f1: 0.9955 - val_loss: 1.6008 - val_f1: 0.6878\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0056 - f1: 0.9973 - val_loss: 1.6649 - val_f1: 0.6868\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0046 - f1: 0.9976 - val_loss: 1.6958 - val_f1: 0.6919\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0029 - f1: 0.9988 - val_loss: 1.6826 - val_f1: 0.6909\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0091 - f1: 0.9952 - val_loss: 1.5790 - val_f1: 0.6619\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0073 - f1: 0.9962 - val_loss: 1.5967 - val_f1: 0.7067\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0038 - f1: 0.9979 - val_loss: 1.5769 - val_f1: 0.6966\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0050 - f1: 0.9978 - val_loss: 1.6300 - val_f1: 0.6903\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9976 - val_loss: 1.6812 - val_f1: 0.6847\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0115 - f1: 0.9960 - val_loss: 1.6374 - val_f1: 0.6955\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0098 - f1: 0.9957 - val_loss: 1.6161 - val_f1: 0.6905\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0094 - f1: 0.9952 - val_loss: 1.5506 - val_f1: 0.6812\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0065 - f1: 0.9969 - val_loss: 1.5503 - val_f1: 0.6907\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0060 - f1: 0.9976 - val_loss: 1.5778 - val_f1: 0.6764\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0035 - f1: 0.9990 - val_loss: 1.6959 - val_f1: 0.6976\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0085 - f1: 0.9967 - val_loss: 1.5759 - val_f1: 0.6954\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0068 - f1: 0.9969 - val_loss: 1.5820 - val_f1: 0.6880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0069 - f1: 0.9974 - val_loss: 1.6033 - val_f1: 0.7019\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0091 - f1: 0.9948 - val_loss: 1.5196 - val_f1: 0.6879\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0048 - f1: 0.9976 - val_loss: 1.5322 - val_f1: 0.6974\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0062 - f1: 0.9979 - val_loss: 1.6260 - val_f1: 0.7028\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0042 - f1: 0.9978 - val_loss: 1.6749 - val_f1: 0.6894\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0068 - f1: 0.9959 - val_loss: 1.7366 - val_f1: 0.6923\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0095 - f1: 0.9955 - val_loss: 1.5707 - val_f1: 0.6918\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0061 - f1: 0.9964 - val_loss: 1.6728 - val_f1: 0.7050\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0068 - f1: 0.9959 - val_loss: 1.6079 - val_f1: 0.6704\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0085 - f1: 0.9943 - val_loss: 1.5862 - val_f1: 0.6832\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0097 - f1: 0.9972 - val_loss: 1.6025 - val_f1: 0.6832\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0016 - f1: 0.9998 - val_loss: 1.6799 - val_f1: 0.6967\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0026 - f1: 0.9986 - val_loss: 1.7093 - val_f1: 0.6949\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0121 - f1: 0.9945 - val_loss: 1.6484 - val_f1: 0.6997\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0083 - f1: 0.9952 - val_loss: 1.6945 - val_f1: 0.7050\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0154 - f1: 0.9943 - val_loss: 1.4473 - val_f1: 0.6946\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0062 - f1: 0.9959 - val_loss: 1.5435 - val_f1: 0.6684\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0073 - f1: 0.9976 - val_loss: 1.5042 - val_f1: 0.6694\n",
      "Train on 2909 samples, validate on 967 samples\n",
      "Epoch 1/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0094 - f1: 0.9959 - val_loss: 1.5069 - val_f1: 0.6778\n",
      "Epoch 2/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0015 - f1: 0.9998 - val_loss: 1.5822 - val_f1: 0.6850\n",
      "Epoch 3/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0012 - f1: 0.9997 - val_loss: 1.6480 - val_f1: 0.6878\n",
      "Epoch 4/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0055 - f1: 0.9978 - val_loss: 1.6562 - val_f1: 0.6853\n",
      "Epoch 5/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0056 - f1: 0.9974 - val_loss: 1.7459 - val_f1: 0.6906\n",
      "Epoch 6/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0014 - f1: 0.9995 - val_loss: 1.7943 - val_f1: 0.6823\n",
      "Epoch 7/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0027 - f1: 0.9981 - val_loss: 1.8591 - val_f1: 0.6864\n",
      "Epoch 8/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0025 - f1: 0.9986 - val_loss: 1.8447 - val_f1: 0.6943\n",
      "Epoch 9/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0086 - f1: 0.9976 - val_loss: 1.8419 - val_f1: 0.7009\n",
      "Epoch 10/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0061 - f1: 0.9973 - val_loss: 1.7630 - val_f1: 0.6850\n",
      "Epoch 11/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0105 - f1: 0.9952 - val_loss: 1.6165 - val_f1: 0.6821\n",
      "Epoch 12/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0030 - f1: 0.9985 - val_loss: 1.7809 - val_f1: 0.6942\n",
      "Epoch 13/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0060 - f1: 0.9974 - val_loss: 1.7290 - val_f1: 0.6915\n",
      "Epoch 14/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0116 - f1: 0.9952 - val_loss: 1.6103 - val_f1: 0.6780\n",
      "Epoch 15/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0085 - f1: 0.9972 - val_loss: 1.6695 - val_f1: 0.6733\n",
      "Epoch 16/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0075 - f1: 0.9967 - val_loss: 1.7036 - val_f1: 0.6845\n",
      "Epoch 17/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0026 - f1: 0.9986 - val_loss: 1.7904 - val_f1: 0.6999\n",
      "Epoch 18/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0099 - f1: 0.9961 - val_loss: 1.6717 - val_f1: 0.6750\n",
      "Epoch 19/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0026 - f1: 0.9993 - val_loss: 1.7368 - val_f1: 0.6895\n",
      "Epoch 20/20\n",
      "2909/2909 [==============================] - 3s 1ms/step - loss: 0.0052 - f1: 0.9974 - val_loss: 1.7594 - val_f1: 0.6808\n",
      "213/213 [==============================] - 0s 349us/step\n"
     ]
    }
   ],
   "source": [
    "DATA_SPLIT_SEED = 2018\n",
    "clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)\n",
    "\n",
    "train_meta = np.zeros(Y.shape)\n",
    "test_meta = np.zeros((X_test.shape[0], num_classes))\n",
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=DATA_SPLIT_SEED).split(X, encoded_Y))\n",
    "for idx, (train_idx, valid_idx) in enumerate(splits):\n",
    "        X_train = X[train_idx]\n",
    "        y_train = encoded_Y[train_idx]\n",
    "        X_val = X[valid_idx]\n",
    "        y_val = encoded_Y[valid_idx]\n",
    "        \n",
    "        Y_train_encoded = utils.to_categorical(y_train, num_classes)\n",
    "        Y_val_encoded = utils.to_categorical(y_val, num_classes)\n",
    "        \n",
    "        model = model_lstm_atten()\n",
    "        pred_val_y, pred_test_y = train_pred(model, X_train, Y_train_encoded, X_val, Y_val_encoded, epochs = 8, callback = [clr,])\n",
    "        np.add(test_meta, pred_test_y / len(splits), out=test_meta, casting=\"unsafe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_pred = np.argmax(test_meta, axis=1)\n",
    "test['predictions'] = le.inverse_transform(encoded_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coinfusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from visualization.confusion_matrix_pretty_print import plot_confusion_matrix_from_data\n",
    "\n",
    "columns = []\n",
    "annot = True\n",
    "cmap = 'Oranges'\n",
    "fmt = '.2f'\n",
    "lw = 0.5\n",
    "cbar = False\n",
    "show_null_values = 2\n",
    "pred_val_axis = 'y'\n",
    "# size::\n",
    "fz = 12;\n",
    "figsize = [9, 9];\n",
    "if len(test[\"subtask_c\"]) > 10:\n",
    "    fz = 9;\n",
    "    figsize = [14, 14];\n",
    "plot_confusion_matrix_from_data(test[\"subtask_c\"], test['predictions'], columns,\n",
    "                                    annot, cmap, fmt, fz, lw, cbar, figsize, show_null_values, pred_val_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6619718309859155"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test[\"subtask_c\"], test['predictions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentence_similarity_3.6]",
   "language": "python",
   "name": "conda-env-sentence_similarity_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
