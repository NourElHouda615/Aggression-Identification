{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offensive Language Identification\n",
    "\n",
    "This is the task 'a' in Task 6: OffensEval: Identifying and Categorizing Offensive Language in Social Media in SemEval 2019. We have explored how a RNN with attention + cyclic learning rate can be used in Offensive language identification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100  # maBaselinegic number - length to truncate sequences of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training and testing files and Changing case of the tweets to lower case, since the embedding model only has lower case words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"Data/offenseval-training-v1.tsv\", sep='\\t')\n",
    "test_df = pd.read_csv(\"Data/test.tsv\")\n",
    "\n",
    "train_df[\"tweet\"] = train_df[\"tweet\"].str.lower()\n",
    "test_df[\"tweet\"] = test_df[\"tweet\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the puncutation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "test_df\n",
    "\n",
    "train_df[\"tweet\"] = train_df[\"tweet\"].apply(lambda x: clean_text(x))\n",
    "test_df[\"tweet\"] = test_df[\"tweet\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12030</th>\n",
       "      <td>51937</td>\n",
       "      <td>@ user damn is she ugly !  !  !  !</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11682</th>\n",
       "      <td>78293</td>\n",
       "      <td>@ user no way !  she is !</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9235</th>\n",
       "      <td>95376</td>\n",
       "      <td>@ user  @ user  @ user  @ user  @ user  @ use...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>16502</td>\n",
       "      <td># mogg :   # jacobreesmogg calls for  @ user ...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>91767</td>\n",
       "      <td>@ user  @ user  @ user  @ user  @ user i thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  subtask_a  \\\n",
       "12030  51937                @ user damn is she ugly !  !  !  !           1   \n",
       "11682  78293                         @ user no way !  she is !           0   \n",
       "9235   95376   @ user  @ user  @ user  @ user  @ user  @ use...          0   \n",
       "10971  16502   # mogg :   # jacobreesmogg calls for  @ user ...          1   \n",
       "5236   91767   @ user  @ user  @ user  @ user  @ user i thin...          1   \n",
       "\n",
       "      subtask_b subtask_c  \n",
       "12030       TIN       IND  \n",
       "11682       NaN       NaN  \n",
       "9235        NaN       NaN  \n",
       "10971       TIN       IND  \n",
       "5236        TIN       IND  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.1)\n",
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11916.000000\n",
       "mean        29.273582\n",
       "std         20.435321\n",
       "min          3.000000\n",
       "25%         14.000000\n",
       "50%         24.000000\n",
       "75%         41.000000\n",
       "max        163.000000\n",
       "Name: tweet, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minor eda: average question length (in words) is 22  , majority are under 18 words\n",
    "train_df.tweet.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:32, 23705.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embedding setup\n",
    "# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# \n",
    "embeddings_index = {}\n",
    "f = open('/data/glove/glove.840B.300d.txt')\n",
    "# f = open('../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "STOP_WORDS = \"\\\" \\' [ ] . , ! : ; ?\".split(\" \")\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "        # return [w.lower() for w in words if w not in stop_words and w != '' and w != ' ']\n",
    "    return [w.lower() for w in words if w != '' and w != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [00:00<00:00, 9959.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert values to embeddings\n",
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = basic_tokenizer(text[:-1])[:SEQ_LEN]\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "    embeds+= [empyt_emb] * (SEQ_LEN - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"tweet\"])])\n",
    "val_y = np.array(val_df[\"subtask_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data providers\n",
    "batch_size = 32\n",
    "\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"subtask_a\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Input,Dropout\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinning everything together LSTM, LSTM + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tharindu/anaconda3/envs/sentence_similarity_3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(SEQ_LEN,300 ))\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(inp)\n",
    "x = Bidirectional(LSTM(64,return_sequences=True))(x)\n",
    "x = Attention(SEQ_LEN)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tharindu/anaconda3/envs/sentence_similarity_3.6/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 80s 80ms/step - loss: 0.4748 - acc: 0.7773 - val_loss: 0.4523 - val_acc: 0.8029\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.3956 - acc: 0.8195 - val_loss: 0.5003 - val_acc: 0.7704\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.3084 - acc: 0.8681 - val_loss: 0.7141 - val_acc: 0.7734\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 80s 80ms/step - loss: 0.1940 - acc: 0.9242 - val_loss: 0.9553 - val_acc: 0.7538\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 77s 77ms/step - loss: 0.1079 - acc: 0.9609 - val_loss: 1.3292 - val_acc: 0.7515\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 77s 77ms/step - loss: 0.0674 - acc: 0.9765 - val_loss: 1.3947 - val_acc: 0.7568\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0463 - acc: 0.9839 - val_loss: 1.4083 - val_acc: 0.7742\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0358 - acc: 0.9882 - val_loss: 1.4000 - val_acc: 0.7545\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0316 - acc: 0.9893 - val_loss: 1.4529 - val_acc: 0.7485\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0274 - acc: 0.9904 - val_loss: 1.6885 - val_acc: 0.7538\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 80s 80ms/step - loss: 0.0242 - acc: 0.9920 - val_loss: 1.7733 - val_acc: 0.7440\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0258 - acc: 0.9908 - val_loss: 1.8067 - val_acc: 0.7681\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 80s 80ms/step - loss: 0.0172 - acc: 0.9937 - val_loss: 1.6584 - val_acc: 0.7545\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0181 - acc: 0.9938 - val_loss: 1.9703 - val_acc: 0.7666\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0210 - acc: 0.9924 - val_loss: 1.7955 - val_acc: 0.7757\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0183 - acc: 0.9938 - val_loss: 1.7568 - val_acc: 0.7636\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0181 - acc: 0.9934 - val_loss: 1.7648 - val_acc: 0.7576\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0151 - acc: 0.9948 - val_loss: 1.8439 - val_acc: 0.7598\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 77s 77ms/step - loss: 0.0183 - acc: 0.9938 - val_loss: 1.7498 - val_acc: 0.7757\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0114 - acc: 0.9959 - val_loss: 1.9700 - val_acc: 0.7621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9d8a16c18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_df)\n",
    "model.fit_generator(mg, epochs=20,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the predictions in to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te = (np.array(all_preds) > 0.35).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 78, 92, 148)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_df['predictions'] = y_te\n",
    "tn, fp, fn, tp = confusion_matrix(test_df[\"subtask_a\"], test_df['predictions']).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8023255813953488"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_df[\"subtask_a\"], test_df['predictions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentence_similarity_3.6]",
   "language": "python",
   "name": "conda-env-sentence_similarity_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
