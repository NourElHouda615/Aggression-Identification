{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 100  # magic number - length to truncate sequences of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>65338</td>\n",
       "      <td>@USER @USER says social housing is good and in...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>57305</td>\n",
       "      <td>@USER @USER @USER @USER @USER @USER @USER Lie ...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>35710</td>\n",
       "      <td>@USER I’m jealous</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10932</th>\n",
       "      <td>26634</td>\n",
       "      <td>@USER @USER Doesn't mean you are just fighting...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>16278</td>\n",
       "      <td>@USER what the actual fuck..ill report them th...</td>\n",
       "      <td>1</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  subtask_a  \\\n",
       "1768   65338  @USER @USER says social housing is good and in...          1   \n",
       "765    57305  @USER @USER @USER @USER @USER @USER @USER Lie ...          1   \n",
       "3770   35710                                  @USER I’m jealous          1   \n",
       "10932  26634  @USER @USER Doesn't mean you are just fighting...          0   \n",
       "1026   16278  @USER what the actual fuck..ill report them th...          1   \n",
       "\n",
       "      subtask_b subtask_c  \n",
       "1768        TIN       IND  \n",
       "765         TIN       IND  \n",
       "3770        TIN       IND  \n",
       "10932       NaN       NaN  \n",
       "1026        TIN       IND  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Data/offenseval-training-v1.tsv\", sep='\\t')\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)\n",
    "train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11916.000000\n",
       "mean        22.303038\n",
       "std         15.092471\n",
       "min          2.000000\n",
       "25%         10.000000\n",
       "50%         18.000000\n",
       "75%         32.000000\n",
       "max        103.000000\n",
       "Name: tweet, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#minor eda: average question length (in words) is 22  , majority are under 18 words\n",
    "train_df.tweet.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:32, 23807.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# embedding setup\n",
    "# Source https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "# \n",
    "embeddings_index = {}\n",
    "f = open('/data/glove/glove.840B.300d.txt')\n",
    "# f = open('../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(\"([.,!?\\\"':;)(])\")\n",
    "_DIGIT_RE = re.compile(br\"\\d\")\n",
    "STOP_WORDS = \"\\\" \\' [ ] . , ! : ; ?\".split(\" \")\n",
    "def basic_tokenizer(sentence):\n",
    "    \"\"\"Very basic tokenizer: split the sentence into a list of tokens.\"\"\"\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "        # return [w.lower() for w in words if w not in stop_words and w != '' and w != ' ']\n",
    "    return [w.lower() for w in words if w != '' and w != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1324/1324 [00:00<00:00, 8777.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert values to embeddings\n",
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = basic_tokenizer(text[:-1])[:SEQ_LEN]\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "    embeds+= [empyt_emb] * (SEQ_LEN - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "# train_vects = [text_to_array(X_text) for X_text in tqdm(train_df[\"question_text\"])]\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in tqdm(val_df[\"tweet\"])])\n",
    "val_y = np.array(val_df[\"subtask_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data providers\n",
    "batch_size = 32\n",
    "\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"subtask_a\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM, Dense, Bidirectional, Input,Dropout\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(SEQ_LEN,300 ))\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(inp)\n",
    "x = Bidirectional(LSTM(64,return_sequences=True))(x)\n",
    "x = Attention(SEQ_LEN)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "# x = Dropout(0.25)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0091 - acc: 0.9967 - val_loss: 2.1464 - val_acc: 0.7455\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 80s 80ms/step - loss: 0.0138 - acc: 0.9957 - val_loss: 2.3171 - val_acc: 0.7228\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0158 - acc: 0.9949 - val_loss: 1.6340 - val_acc: 0.7568\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 80s 80ms/step - loss: 0.0076 - acc: 0.9971 - val_loss: 2.2193 - val_acc: 0.7462\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0113 - acc: 0.9960 - val_loss: 2.0828 - val_acc: 0.7409\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0113 - acc: 0.9961 - val_loss: 2.0757 - val_acc: 0.7508\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0080 - acc: 0.9971 - val_loss: 1.9067 - val_acc: 0.7455\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 82s 82ms/step - loss: 0.0099 - acc: 0.9965 - val_loss: 2.0176 - val_acc: 0.7651\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 89s 89ms/step - loss: 0.0050 - acc: 0.9982 - val_loss: 2.2507 - val_acc: 0.7560\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 91s 91ms/step - loss: 0.0103 - acc: 0.9961 - val_loss: 2.0762 - val_acc: 0.7545\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 89s 89ms/step - loss: 0.0072 - acc: 0.9974 - val_loss: 2.3264 - val_acc: 0.7598\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 82s 82ms/step - loss: 0.0101 - acc: 0.9964 - val_loss: 2.2473 - val_acc: 0.7576\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0091 - acc: 0.9967 - val_loss: 1.9634 - val_acc: 0.7545\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.0080 - acc: 0.9971 - val_loss: 2.2040 - val_acc: 0.7613\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0089 - acc: 0.9970 - val_loss: 2.0685 - val_acc: 0.7606\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0065 - acc: 0.9976 - val_loss: 2.4910 - val_acc: 0.7258\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 82s 82ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 2.3639 - val_acc: 0.7402\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.0046 - acc: 0.9987 - val_loss: 2.5021 - val_acc: 0.7477\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 81s 81ms/step - loss: 0.0046 - acc: 0.9985 - val_loss: 2.4967 - val_acc: 0.7432\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 81s 81ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 1.1975 - val_acc: 0.7424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ce4630320>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_df)\n",
    "model.fit_generator(mg, epochs=20,\n",
    "                    steps_per_epoch=1000,\n",
    "                    validation_data=(val_vects, val_y),\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  3.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# prediction part\n",
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "test_df = pd.read_csv(\"Data/test.tsv\")\n",
    "\n",
    "all_preds = []\n",
    "for x in tqdm(batch_gen(test_df)):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te = (np.array(all_preds) > 0.35).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(542, 78, 92, 148)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_df['predictions'] = y_te\n",
    "tn, fp, fn, tp = confusion_matrix(test_df[\"subtask_a\"], test_df['predictions']).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8023255813953488"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_df[\"subtask_a\"], test_df['predictions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentence_similarity_3.6]",
   "language": "python",
   "name": "conda-env-sentence_similarity_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
